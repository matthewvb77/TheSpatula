{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import csv\n",
    "import re\n",
    "import json   \n",
    "import requests\n",
    "import mysql.connector\n",
    "import traceback\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "reddit = praw.Reddit()\n",
    "\n",
    "class StockPost(object):\n",
    "    def __init__(self, postID, postURL, ups, downs, numComments, stock, date):\n",
    "        self.postID = postID\n",
    "        self.url = postURL\n",
    "        self.stock = stock\n",
    "        self.ups = ups\n",
    "        self.downs = downs\n",
    "        self.numComments = numComments\n",
    "        self.date = date\n",
    "    \n",
    "    def jsonEnc(self):\n",
    "      return {'stock': self.stock, 'postID': self.postID, 'postURL': self.url, 'ups': self.ups, 'downs': self.downs, 'numComments': self.numComments}\n",
    "\n",
    "def jsonDefEncoder(obj):\n",
    "    if hasattr(obj, 'jsonEnc'):\n",
    "        return obj.jsonEnc()\n",
    "    else: #some default behavior\n",
    "        return obj.__dict__\n",
    "\n",
    " ### DATABASE FUNCTIONS ###\n",
    "\n",
    "    # returns connection object #\n",
    "def connect_to_db(db_name):\n",
    "    cnx = mysql.connector.connect(\n",
    "    user='root',\n",
    "    password='chalkHorseMountain',\n",
    "    host='localhost',\n",
    "    database=db_name\n",
    "    )\n",
    "    return cnx\n",
    "    \n",
    "    # returns boolean #\n",
    "def table_exists(cursor, tbl_name):\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = DATABASE()\n",
    "        AND table_name = \\\"\"\"\" + tbl_name + \"\"\"\\\";\n",
    "    \"\"\")\n",
    "    \n",
    "    if cursor.fetchone()[0] == 1:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    "class SubredditScraper:\n",
    "\n",
    "    def __init__(self, sub, sort='new', lim=900):\n",
    "        self.sub = sub\n",
    "        self.sort = sort\n",
    "        self.lim = lim\n",
    "\n",
    "        #print(\n",
    "            #f'SubredditScraper instance created with values '\n",
    "            #f'sub = {sub}, sort = {sort}, lim = {lim}')\n",
    "\n",
    "    def set_sort(self):\n",
    "        if self.sort == 'new':\n",
    "            return self.sort, reddit.subreddit(self.sub).new(limit=self.lim)\n",
    "        elif self.sort == 'top':\n",
    "            return self.sort, reddit.subreddit(self.sub).top(limit=self.lim)\n",
    "        elif self.sort == 'hot':\n",
    "            return self.sort, reddit.subreddit(self.sub).hot(limit=self.lim)\n",
    "        else:\n",
    "            self.sort = 'hot'\n",
    "            print('Sort method was not recognized, defaulting to hot.')\n",
    "            return self.sort, reddit.subreddit(self.sub).hot(limit=self.lim)\n",
    "\n",
    "    def get_posts(self):\n",
    "\n",
    "        stockTickers = {}\n",
    "        with open('./../Tickers/tickers_crypto.csv', mode='r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            for row in reader:\n",
    "                stockTickers[row[0].split(',')[0]] = {}\n",
    "        \"\"\"Get unique posts from a specified subreddit.\"\"\"\n",
    "\n",
    "        # Attempt to specify a sorting method.\n",
    "        sort, subreddit = self.set_sort()\n",
    "\n",
    "        print(f'Collecting information from r/{self.sub}.')\n",
    "        \n",
    "        ## Search posts for tickers ##\n",
    "        relevantPosts = []\n",
    "        subreddit = list(subreddit)\n",
    "        for i in tqdm(range(len(subreddit)), desc=\"[1/2] Scraping Posts\"):\n",
    "            post = subreddit[i]\n",
    "            if post.link_flair_text != 'Meme':\n",
    "                for stock in stockTickers.keys():\n",
    "                    try:\n",
    "                        if(re.search(r\"\\s+\\$?\" + stock + r\"\\$?\\s+\", post.selftext) or re.search(r\"\\s+\\$?\" + stock + r\"\\$?\\s+\",  post.title)):\n",
    "                            stockTickers[stock][post.id] = StockPost(post.id, post.permalink, post.ups, post.downs, post.num_comments, stock, post.created_utc)\n",
    "                    except:\n",
    "                        print(f\"This Ticker threw an exception: {stock}\")\n",
    "                        traceback.print_exc()\n",
    "                    \n",
    "        for stock in stockTickers.keys():\n",
    "            if (len(stockTickers[stock]) > 0):\n",
    "                for post in stockTickers[stock]:\n",
    "                    \n",
    "                    relevantPosts.append(stockTickers[stock][post]) \n",
    "        #json_object = json.dumps(relevantPosts, default=jsonDefEncoder, indent = 4)   \n",
    "        #print(json_object)\n",
    "        \n",
    "         ## Upload data to db ##\n",
    "        cnx = connect_to_db(\"TheSpatula\")\n",
    "        mycursor = cnx.cursor()\n",
    "        assert mycursor\n",
    "        assert table_exists(mycursor, \"reddit\")\n",
    "        \n",
    "        for x in tqdm(range(len(relevantPosts)), desc=\"[2/2] Updating Database\"):\n",
    "            post = relevantPosts[x]\n",
    "            num_votes = post.ups + post.downs\n",
    "            \n",
    "            # get created_date and convert from utc to local time\n",
    "            utc_stamp = post.date\n",
    "            utc = datetime.utcfromtimestamp(utc_stamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            from_zone = tz.tzutc()\n",
    "            to_zone = tz.tzlocal()\n",
    "            utc = datetime.strptime(utc, '%Y-%m-%d %H:%M:%S')\n",
    "            utc = utc.replace(tzinfo=from_zone)\n",
    "            date_posted = utc.astimezone(to_zone).date()\n",
    "            \n",
    "            ## Add post, if it exists already, update post ##\n",
    "            mycursor.execute(f\"\"\"\n",
    "            INSERT INTO reddit (post_id, symbol, num_comments, num_votes, date_posted) \n",
    "            VALUES(\"{post.postID}\", \"{post.stock}\", {post.numComments}, {num_votes}, \"{date_posted}\")\n",
    "            ON DUPLICATE KEY UPDATE num_comments={post.numComments}, num_votes={num_votes}, date_posted=\"{date_posted}\"\n",
    "            ;\"\"\")\n",
    "            \n",
    "            cnx.commit()\n",
    "            \n",
    "            \n",
    "## get_posts() every subreddit with 10000 post limit ##\n",
    "def deep_scrape():\n",
    "    subreddits = [\"CryptoCurrency\", \"CryptoMoonShots\", \"CryptoMarkets\", \"Crypto_com\", \"wallstreetbets\", \"Wallstreetbetsnew\", \"stocks\", \"RobinHoodPennyStocks\", \"pennystocks\", \"weedstocks\", \"trakstocks\", \"ausstocks\", \"shroomstocks\", \"Canadapennystocks\"]\n",
    "    \n",
    "    for x in tqdm(range(len(subreddits)), desc=\"DEEP SCRAPE\"):\n",
    "        sub = subreddits[x]\n",
    "        SubredditScraper(sub, lim=10000, sort='new').get_posts()\n",
    "        SubredditScraper(sub, lim=10000, sort='hot').get_posts()\n",
    "        SubredditScraper(sub, lim=10000, sort='top').get_posts()\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    #deep_scrape()\n",
    "    SubredditScraper(sub, lim=20, sort='hot').get_posts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
